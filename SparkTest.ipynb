{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook for Data Engineering Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.output_text { font-size: 14px; }.CodeMirror { font-size: 14px; }.container { width: 90% !important; font-size: 14px; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## This is a helper script I have been running for a long time to provide as much width as needed (and is practical)\n",
    "## Also provides simple mods for cell/output text size and several other hacks\n",
    "\n",
    "%run display.py\n",
    "get_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import UserDict\n",
    "# import types as types_from_types\n",
    "# import pyspark.sql.functions as F\n",
    "# from pyspark.sql import DataFrame, Row\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_conf = {\n",
    "    \"spark.plugins\": \"org.apache.spark.sql.connect.SparkConnectPlugin\"\n",
    "}\n",
    "overwrite_conf = {\n",
    "    \"spark.master\": \"master\",\n",
    "}\n",
    "def create_conf(**kwargs: Any) -> SparkConf:\n",
    "    sparkConf = SparkConf(**kwargs)\n",
    "    for k, v in overwrite_conf.items():\n",
    "        sparkConf.set(k, v)\n",
    "    for k, v in default_conf.items():\n",
    "        if not sparkConf.contains(k):\n",
    "            sparkConf.set(k, v)\n",
    "    return sparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fun for another time...\n",
    "\n",
    "local_conf = {\n",
    "    \"spark.cores.max\": 6,\n",
    "    \"spark.driver.cores\": 2,\n",
    "    \"spark.driver.memory\": \"4g\",\n",
    "    \"spark.executor.memory\": \"3g\",\n",
    "    \"spark.executor.cores\": 3,\n",
    "    \"spark.logConf\": True,\n",
    "    \"spark.shuffle.service.enabled\": True,\n",
    "    \"spark.dynamicAllocation.enabled\": True,\n",
    "    \"spark.dynamicAllocation.minExecutors\": 4,\n",
    "    \"spark.dynamicAllocation.maxExecutors\": 6,\n",
    "    \"spark.default.parallelism\": 16\n",
    "}\n",
    "\n",
    "conf = SparkConf(local_conf)\n",
    "conf.setMaster(\"local[*]\").setAppName(\"DataEngineeringTest\")\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sparkContext=sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_doc_num                 = \"DocumentNumber\"\n",
    "col_doc_dt                  = \"DocumentDate\"\n",
    "col_doc_type                = \"DocumentType\"\n",
    "col_ref_to_doc_num          = \"RefersToDocumentNumber\"\n",
    "col_ref_to_doc_year         = \"RefersToDocumentYear\"\n",
    "col_remarks                 = \"Remarks\"\n",
    "\n",
    "col_formatted_doc_dt        = \"FormattedDocumentDate\"\n",
    "\n",
    "col_ref_year_ref_num_arr    = \"RefYearRefNumArray\"\n",
    "col_ref_year_num_concat     = \"RefYearNumConcat\"\n",
    "\n",
    "col_year_num_arr            = \"YearNumArray\"\n",
    "col_year_num_concat         = \"YearNumConcat\"\n",
    "\n",
    "col_all                     = col('*')\n",
    "\n",
    "key_types_jrt               = ('J','T','R')\n",
    "key_types_abc               = ('A','B','C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_df(df, n=25, orderby_cols=(col_doc_type, col_ref_to_doc_num)):\n",
    "    df.orderBy(*orderby_cols).show(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('data-engineer-interview-data.csv', header=True, inferSchema=True, samplingRatio=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \"Make\" the date from scratch using the existing repr, need to be careful of M/d/yyy vs. MM/dd/yyyy (e.g.)\n",
    "\n",
    "df = df.select(\"*\", split(col_doc_dt, \"/\").alias('dt')).withColumn(col_formatted_doc_dt,\n",
    "        to_date(make_date(month=col(\"dt\").__getitem__(0), day=col(\"dt\").__getitem__(1), year=col(\"dt\").__getitem__(2)), 'MM/dd/yyyy'))\n",
    "df = df.orderBy(col_doc_type, col_doc_num).drop('dt').fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Since the two \"Reference\" cols had years and nums in both, I created this func() to ensure the year was first in the concatenation of \"year.num\" being handy later for\n",
    "## the joins. I explored the data quite a bit and saw that the only length 4 entries between the two \"Reference\" cols were also within the min/max range of years that \n",
    "## substiantiated the \"DocumentDate\" field so, for this assignment I made the assumption that they had been entered in the opposing fields occasionally, as mentioned\n",
    "\n",
    "def concat_cols(df, first_col, second_col, final_col_name):\n",
    "    concat_ref_col = when(length(first_col) == 4,\n",
    "                        concat_ws('.', first_col, second_col)).otherwise(\n",
    "                            concat_ws('.', second_col, first_col))\n",
    "    return df.withColumn(final_col_name, concat_ref_col)\n",
    "\n",
    "df = concat_cols(df, col(col_ref_to_doc_num), col(col_ref_to_doc_year), col_ref_year_num_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = concat_cols(df, year(col_formatted_doc_dt), col(col_doc_num), col_year_num_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_keys = {row[0]: row[1] for row in df.groupBy(col(col_doc_type)).count().collect()}.keys()\n",
    "others = sorted([t for t in type_keys if t not in key_types_jrt])\n",
    "\n",
    "col_with_temp = lambda c: (c, f\"{c}_TEMP\")\n",
    "only_temp_col = lambda c: col_with_temp(c)[-1]\n",
    "temp_for_col = lambda c: (f\"{c}_TEMP\", c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doctype_with_temp_col(df, doc_type_chars, cur_col_to_temp):\n",
    "    return df.where(col(col_doc_type).isin(doc_type_chars)).withColumnRenamed(*col_with_temp(cur_col_to_temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 9, 28, 89)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Split off DataFrames per needs outlined in the docs, using helpers to make subsequent joins easier\n",
    "\n",
    "dfj = get_doctype_with_temp_col(df, 'J', col_year_num_concat)\n",
    "dft= get_doctype_with_temp_col(df, 'T', col_ref_year_num_concat)\n",
    "dfabc = get_doctype_with_temp_col(df, others, col_year_num_concat)\n",
    "dfr = get_doctype_with_temp_col(df, 'R', col_ref_year_num_concat)\n",
    "dfj.count(), dft.count(), dfr.count(), dfabc.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dfj count before anti_join: [20], dfj_applied count after: [13]\n"
     ]
    }
   ],
   "source": [
    "## Used anti joins, which is ideal for discounting rows from a larger spread, but it works fine this way also with union reconstruction later\n",
    "\n",
    "dfj_applied = dfj.join(dft, on=[dfj[only_temp_col(col_year_num_concat)] == dft[only_temp_col(col_ref_year_num_concat)]], how=\"left_anti\")\n",
    "print(f\"dfj count before anti_join: [{dfj.count()}], dfj_applied count after: [{dfj_applied.count()}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count:  7\n",
      "+--------------+------------+------------+----------------------+--------------------+-------+---------------------+----------------+------------------+\n",
      "|DocumentNumber|DocumentDate|DocumentType|RefersToDocumentNumber|RefersToDocumentYear|Remarks|FormattedDocumentDate|RefYearNumConcat|YearNumConcat_TEMP|\n",
      "+--------------+------------+------------+----------------------+--------------------+-------+---------------------+----------------+------------------+\n",
      "|            37|   7/24/1987|           J|                  null|                null|       |           1987-07-24|                |           1987.37|\n",
      "|            52|  12/22/2006|           J|                  null|                null|       |           2006-12-22|                |           2006.52|\n",
      "|            73|  11/13/2003|           J|                  null|                null|       |           2003-11-13|                |           2003.73|\n",
      "|           125|  12/16/1993|           J|                  null|                null|       |           1993-12-16|                |          1993.125|\n",
      "|           133|  11/16/1995|           J|                  null|                null|       |           1995-11-16|                |          1995.133|\n",
      "+--------------+------------+------------+----------------------+--------------------+-------+---------------------+----------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## These are the records that matched but due to the join type were not included in the result\n",
    "## And there are indeed a total of [7] records\n",
    "\n",
    "print(\"Row count: \", dfj.subtract(dfj_applied).count())\n",
    "dfj.subtract(dfj_applied).orderBy(col_doc_num).show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dfabc count before anti_join: [89], dfabc_applied count after: [63]\n"
     ]
    }
   ],
   "source": [
    "dfabc_applied = dfabc.join(dfr, on=[dfabc[col_with_temp(col_year_num_concat)[-1]] == dfr[col_with_temp(col_ref_year_num_concat)[-1]]], how=\"left_anti\")\n",
    "print(f\"dfabc count before anti_join: [{dfabc.count()}], dfabc_applied count after: [{dfabc_applied.count()}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count:  26\n",
      "+--------------+------------+------------+----------------------+--------------------+-----------+---------------------+----------------+------------------+\n",
      "|DocumentNumber|DocumentDate|DocumentType|RefersToDocumentNumber|RefersToDocumentYear|    Remarks|FormattedDocumentDate|RefYearNumConcat|YearNumConcat_TEMP|\n",
      "+--------------+------------+------------+----------------------+--------------------+-----------+---------------------+----------------+------------------+\n",
      "|             1|   10/2/2000|           A|                  null|                null|$10,000.00 |           2000-10-02|                |            2000.1|\n",
      "|             9|  12/19/1988|           A|                  null|                null|           |           1988-12-19|                |            1988.9|\n",
      "|            28|  10/14/1994|           A|                  null|                null|           |           1994-10-14|                |           1994.28|\n",
      "|            55|   5/19/1989|           A|                  null|                null|           |           1989-05-19|                |           1989.55|\n",
      "|            65|    7/2/1997|           A|                  null|                null|           |           1997-07-02|                |           1997.65|\n",
      "+--------------+------------+------------+----------------------+--------------------+-----------+---------------------+----------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## These are the records that matched but due to the join type were not included in the result\n",
    "## And there are indeed a total of [26] records\n",
    "\n",
    "print(\"Row count: \", dfabc.subtract(dfabc_applied).count())\n",
    "dfabc.subtract(dfabc_applied).orderBy(col_doc_type, col_doc_num).show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Could have done this with a comprehension but the result is still a Potato\n",
    "df_final = (dfj_applied.withColumnRenamed(*temp_for_col(col_year_num_concat)).union(\n",
    "    dfabc_applied.withColumnRenamed(*temp_for_col(col_year_num_concat)).union(\n",
    "        dfr.withColumnRenamed(*temp_for_col(col_ref_year_num_concat)).union(\n",
    "            dft.withColumnRenamed(*temp_for_col(col_ref_year_num_concat))\n",
    "        ))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count of final Dataframe: 113, Row count of initial Dataframe: 146\n",
      "+--------------+------------+------------+----------------------+--------------------+-----------+---------------------+----------------+-------------+\n",
      "|DocumentNumber|DocumentDate|DocumentType|RefersToDocumentNumber|RefersToDocumentYear|    Remarks|FormattedDocumentDate|RefYearNumConcat|YearNumConcat|\n",
      "+--------------+------------+------------+----------------------+--------------------+-----------+---------------------+----------------+-------------+\n",
      "|             1|   10/2/2000|           A|                  null|                null|$10,000.00 |           2000-10-02|                |       2000.1|\n",
      "|             9|  12/19/1988|           A|                  null|                null|           |           1988-12-19|                |       1988.9|\n",
      "|            28|  10/14/1994|           A|                  null|                null|           |           1994-10-14|                |      1994.28|\n",
      "|            55|   5/19/1989|           A|                  null|                null|           |           1989-05-19|                |      1989.55|\n",
      "|            65|    7/2/1997|           A|                  null|                null|           |           1997-07-02|                |      1997.65|\n",
      "+--------------+------------+------------+----------------------+--------------------+-----------+---------------------+----------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Row count of final Dataframe: {df_final.count()}, Row count of initial Dataframe: {df.count()}\")\n",
    "df.subtract(df_final).orderBy(col_doc_type, col_doc_num).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More of a traditional audit trail can be integrated via Py4J to really get after the action happening in each jvm and to record each individual record that is or isn't included in the final result set. It seems kind of silly to set that up just for this. But, in production, we can get as granular as we want. We can debug and log/audit at the task level when needed. For now I've created some auditable data and you can clearly see how the join subtraction results add up to that of the final counts."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
